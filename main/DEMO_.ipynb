{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574b1fb4-ee82-4466-a057-c8daffa18297",
   "metadata": {},
   "source": [
    "# Set up the environnement\n",
    "\n",
    "Used to test BERT on GLUE subsets using huggingface bibli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fa610-9bfb-47bf-8b6c-7c313397fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!git clone \n",
    "!cd MLAProject\n",
    "!cd main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68bc2fa-4ee6-49b1-8a3e-d0d4381d902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Class_Fine_Tuning_V1 import *\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/ProjetMLA/main/modified_transformers')\n",
    "from modified_transformers import BertForSequenceClassification,BertConfig,AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002384d0-24a6-4513-9937-b4db24a65773",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f733a7ff-6d3a-45ee-b346-5b27eb82b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['unacceptable', 'acceptable'], id=None), 'idx': Value(dtype='int32', id=None)}\n",
      "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\", 'label': 1, 'idx': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 3458,\n",
       " 2053,\n",
       " 1281,\n",
       " 112,\n",
       " 189,\n",
       " 4417,\n",
       " 1142,\n",
       " 3622,\n",
       " 117,\n",
       " 1519,\n",
       " 2041,\n",
       " 1103,\n",
       " 1397,\n",
       " 1141,\n",
       " 1195,\n",
       " 17794,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_data = load_dataset('glue','cola')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "print(glue_data['train'].features)\n",
    "print(glue_data[\"train\"][0])\n",
    "\n",
    "token = tokenizer(glue_data['train']['sentence'])\n",
    "token[0].ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b5fa8-32d7-44ac-b7fc-1f1db025856a",
   "metadata": {},
   "source": [
    "The Autokenizer is a class from transformers bibli which transform a sentences into a vector understood by the bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24835452-5fc2-4627-b92c-c080f8c5ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(hidden_size = 1024,\n",
    "                    max_position_embeddings=512,\n",
    "                    num_attention_heads=16,\n",
    "                    num_hidden_layers = 24)\n",
    "model1 = BertForSequenceClassification(config)\n",
    "\n",
    "FineTuning = Fine_Tuning(model1, tokenizer)\n",
    "FineTuning.load_data(dataset_size=5000,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00259b4a-75d2-4d45-b3a0-94d4f0601146",
   "metadata": {},
   "source": [
    "We have created a method named load_data that allows loading the data, splitting the dataset into training, validation, and test sets, and further dividing these datasets into batches.\n",
    "The method take in argument the number of example in the training dataset and the size of a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81cc83-504f-4caf-85d0-18fdf026f56e",
   "metadata": {},
   "source": [
    "# Configuration of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ff2984-0f18-4201-8a09-c278409a600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"adapter\": false,\n",
      "  \"adapter_size\": 16,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"frozen_mode\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "BertConfig {\n",
      "  \"adapter\": true,\n",
      "  \"adapter_size\": 16,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"frozen_mode\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(hidden_size = 1024,\n",
    "                    max_position_embeddings=512,\n",
    "                    num_attention_heads=16,\n",
    "                    num_hidden_layers = 24)\n",
    "model1 = BertForSequenceClassification(config)\n",
    "print(config)\n",
    "config = BertConfig(adapter = True,\n",
    "                    frozen_mode =True,\n",
    "                    hidden_size = 1024,\n",
    "                    max_position_embeddings=512,\n",
    "                    num_attention_heads=16,\n",
    "                    num_hidden_layers = 24)\n",
    "\n",
    "model2 = BertForSequenceClassification(config)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23505695-2fa6-4a4c-be9c-d5bc018c7093",
   "metadata": {},
   "source": [
    "Thanks to the modified transfomers module that we created from the transformers bibli, we can create different bert model with or not adapter layer. The function Config can take in arguments the size of an adapter, if the parameter of bert model have to be frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190e6a7-817a-4faa-a696-beb4b230a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "FineTuning.initialisation(optimizer,epoch=1)\n",
    "FineTuning.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94006b1-bc05-4929-b1e8-ea664927836f",
   "metadata": {},
   "source": [
    "Here, thanks to the class FineTuning, we can train the bert model on the dataset cola for example.\n",
    "We can specify the number of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45d7a3-a8d5-4889-b14e-093171251dfc",
   "metadata": {},
   "source": [
    "# Experiment Execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0dbde-c02c-4c58-9d5e-abac897ab76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /ProjetMLA/main/Fine_Tuning_lr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1283a-34a6-4e3b-bdbf-2ac3439556e3",
   "metadata": {},
   "source": [
    "Fine_Tuning_lr script is designed for fine-tuning the BERT model with different learning rates, aiding in the search for optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678ac6a-3d91-4d8a-8720-2fd7cd9d6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /ProjetMLA/main/Fine_Tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a5cc9-017e-4435-b619-9e88292e8d71",
   "metadata": {},
   "source": [
    "Thanks to Fine_Tuning script we can fine-tune the BERT model on different GLUE datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c750b4-d681-4b38-a535-962a8ebef26f",
   "metadata": {},
   "source": [
    "But watch out the path where you want to save your results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f22b0c-7d47-4302-a40e-033ee255874a",
   "metadata": {},
   "source": [
    "# Loading and Plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392b123-bc32-4321-a618-42bac7a40948",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/ProjetMLA/admin/Resultats/Run_cola/'\n",
    "data = []\n",
    "file_name = []\n",
    "for files in os.listdir(file):\n",
    "    if files.endswith('.npy'):\n",
    "        file_name.append(files)\n",
    "        path = os.path.join(file,files)\n",
    "        data.append(np.load(path,allow_pickle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106eb1d-0e7c-4632-9f8b-d36137f000cc",
   "metadata": {},
   "source": [
    "Here we load the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9df242e-1246-4a6b-9c99-bae3c690075c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m)):\n\u001b[1;32m      2\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(data[i][\u001b[38;5;241m0\u001b[39m],label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(data[i][\u001b[38;5;241m1\u001b[39m],label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    plt.plot(data[i][0],label=\"train_accuracy\")\n",
    "    plt.plot(data[i][1],label=\"val_accuracy\")\n",
    "    plt.title(\"Accuracy on \"+file_name[i])\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "for i in range(len(data)):\n",
    "    plt.plot(data[i][3],label=\"train_loss\")\n",
    "    plt.plot(data[i][4],label=\"val_loss\")\n",
    "    plt.title(\"loss on \"+file_name[i])\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "for i in range(len(data)):\n",
    "    print('Accuracy at the end of fine tuning on {}: {}'.format(file_name[i],data[i][2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc3f33-34db-43e7-9945-4942e0ea86ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
