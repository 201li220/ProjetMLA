{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40863322-0a85-4d17-8302-849e1a0306fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "mismatched keys: 201 != 298. Try using transformers==4.28.1 as this version is known to be compatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     18\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/admin/ProjetMLA/Code_Ugo/BertAdapterNicoV2.py:328\u001b[0m, in \u001b[0;36mBertForSequenceClassification.from_pretrained\u001b[0;34m(cls, model_type, config_args, adaptive_weight_copy)\u001b[0m\n\u001b[1;32m    325\u001b[0m sd_keys_hf \u001b[38;5;241m=\u001b[39m sd_hf\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Check that all keys match between the state dictionary of the custom and pretrained model\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sd_keys_hf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(sd_keys), (\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmismatched keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sd_keys_hf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sd_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry using transformers==4.28.1 as this version is known to be compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m )\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Replace weights in the custom model with the weights from the pretrained model\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sd_keys_hf:\n\u001b[1;32m    335\u001b[0m     \n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# copy over weights if they are the same shape\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: mismatched keys: 201 != 298. Try using transformers==4.28.1 as this version is known to be compatible."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from BertAdapterNicoV2 import BertConfig, BertModel, BertForSequenceClassification\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "print(model)\n",
    "dataset = load_dataset(\"glue\",\"mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4113d1-4891-4c3a-abf7-ff68200052cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
